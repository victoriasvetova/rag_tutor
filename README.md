# RAG Tutor

Простой, но мощный пример реализации конвейера RAG (Retrieval-Augmented Generation) с использованием Ollama, ChromaDB и LangChain. Этот проект предназначен для ответов на вопросы на основе информации из предоставленного вами текстового документа.

## Структура проекта

```
rag_tutor/
├── .env                 # Файл для конфигурации и хранения секретных ключей
├── requirements.txt     # Список необходимых Python библиотек
├── raw_data.txt         # Исходный файл с вашими данными для обработки
├── main.py              # Главный скрипт для запуска приложения
└── src/                 # Папка с исходным кодом проекта
    ├── __init__.py      # Стандартный файл для инициализации Python-модуля
    ├── config.py        # Управление настройками из .env файла
    ├── parser.py        # Модуль для парсинга и обработки raw_data.txt
    ├── database.py      # Модуль для взаимодействия с векторной базой данных ChromaDB
    └── rag_engine.py    # Основная логика RAG: поиск, ранжирование и генерация ответа
```

## Начало работы

Следуйте этим шагам, чтобы настроить и запустить проект локально.

### 1. Предварительные требования

Перед началом убедитесь, что у вас установлены:

*   **Python 3.8+**
*   **Git** для клонирования репозитория
*   **Ollama**: Убедитесь, что приложение [Ollama](https://ollama.com/) установлено и запущено в вашей системе.

### 2. Установка и настройка

**Шаг 1: Клонируйте репозиторий**
Откройте терминал и выполните следующую команду:
```bash
git clone <URL_ВАШЕГО_РЕПОЗИТОРИЯ>
cd rag_tutor
```

**Шаг 2: Создайте и активируйте виртуальное окружение**
Это позволит изолировать зависимости проекта.
*   Для macOS / Linux:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
*   Для Windows:
    ```bash
    python -m venv venv
    venv\Scripts\activate
    ```

**Шаг 3: Установите необходимые библиотеки**
Установите все зависимости, перечисленные в файле `requirements.txt`:
```bash
pip install -r requirements.txt
```
> **Примечание:** Если у вас нет файла `requirements.txt`, создайте его и скопируйте в него следующий список библиотек перед выполнением команды `pip install`:
> ```text
> langchain
> langchain-community
> langchain-ollama
> chromadb
> sentence-transformers
> torch
> python-dotenv
> loguru
> tqdm
> ```

**Шаг 4: Загрузите модели для Ollama**
Убедитесь, что Ollama запущена. Затем скачайте необходимые модели для генерации текста и создания эмбеддингов:
```bash
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

**Шаг 5: Подготовьте данные**
Откройте файл `raw_data.txt` и вставьте в него ваш текстовый контент, который будет использоваться в качестве базы знаний.

**Шаг 6: Настройте переменные окружения**
Создайте файл `.env` в корневой папке проекта. В нем можно указать модели, которые вы хотите использовать (если они отличаются от стандартных в коде):
```env
# Пример содержимого файла .env
EMBEDDING_MODEL="nomic-embed-text"
LLM_MODEL="llama3.1:8b"
```

## Использование

После завершения всех шагов установки и настройки, вы готовы запустить приложение.

Выполните в терминале следующую команду:
```bash
python main.py
```

Скрипт начнет обработку данных из `raw_data.txt`, создаст векторную базу данных и перейдет в режим ожидания ваших вопросов.